<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[【Kaggle泰坦尼克号】用决策树快速的撸一个Baseline]]></title>
    <url>%2F2019%2F01%2F31%2Fkaggle-Titanic%2F</url>
    <content type="text"><![CDATA[1. 引言检验学习成果最快的方式就是去实战，kaggle上提供了各式各样练手和比赛的数据集，”Titanic: Machine Learning from Disaster”就是最经典的入门比赛，既适合经验丰富的Data scientist去深入分析争取top3%的成绩，也适合新手应用数据集对所学习的分类算法来练手。 应用机器学习，千万不要一上来就试图做到完美，先撸一个baseline的model出来，再进行后续的分析步骤，一步步提高。—— Andrew Ng 本篇就是基于决策树模型快速的撸一个baseline model。 选择决策树的原因：模型对数据的要求不高，一般原始数据简单的预处理就能让模型跑起来（如果要得到更高的分数，数据预处理特征工程还是不能少） 2. 泰坦尼克号背景介绍 泰坦尼克号的沉没是历史上最臭名昭著的沉船之一，泰坦尼克号在首航中撞上冰山沉没，2224名乘客和船员中1502人遇难。这一耸人听闻的悲剧震惊了国际社会，并导致了对船舶更严格的安全规定。 我们的任务是运用机器学习的工具，分析船上人员的信息来预测什么样的人能够从船难中活下来。 3. 数据集分析3.1 特征介绍 Variable Definition Key survival 是否生存 0 = No, 1 = Yes pclass 乘客等级(1/2/3等舱位) 1 = 1st, 2 = 2nd, 3 = 3rd sex 性别 Age 年龄 sibsp 堂兄弟/妹个数 parch 父母与小孩个数 ticket 船票信息 fare 票价 cabin 客舱 embarked 登船港口 C = Cherbourg, Q = Queenstown, S = Southampton 3.2 查看缺失值和特征类型In [1]: 1234#导入数据import pandas as pddata = pd.read_csv('Taitanic data/data.csv')data.info() out [1]:12345678910111213141516RangeIndex: 891 entries, 0 to 890Data columns (total 12 columns):PassengerId 891 non-null int64Survived 891 non-null int64Pclass 891 non-null int64Name 891 non-null object &lt;----非数值Sex 891 non-null object &lt;----非数值Age 714 non-null float64 &lt;----有缺失值SibSp 891 non-null int64Parch 891 non-null int64Ticket 891 non-null object &lt;----非数值Fare 891 non-null float64Cabin 204 non-null object &lt;----有缺失值，非数值Embarked 889 non-null object &lt;----有缺失值，非数值dtypes: float64(2), int64(5), object(5)memory usage: 83.6+ KB 通过data.info()和data.head()，我们可以观察出有多少乘客、特征的数据类型和缺失值。 In [2]: 12#观察前5行数据data.head() out [2]: PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 1 0 3 Braund, Mr. Owen Harris male 22.0 1 0 A/5 21171 7.2500 NaN S 1 2 1 1 Cumings, Mrs. John Bradley (Florence Briggs Th... female 38.0 1 0 PC 17599 71.2833 C85 C 2 3 1 3 Heikkinen, Miss. Laina female 26.0 0 0 STON/O2. 3101282 7.9250 NaN S 3 4 1 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.0 1 0 113803 53.1000 C123 S 4 5 0 3 Allen, Mr. William Henry male 35.0 0 0 373450 8.0500 NaN S 决策树模型的输入要求就是特征必须为数值型的数据，且sklearn无法自动的处理缺失值，因此为了尽快的撸出baseline，我们就不过多的进行分析，先把数据调整成符合模型要求的样子。 4. 数据预处理4.1 特征选择 一个正确的数学模型应当在形式上是简单的 —— 吴军，《数学之美》 特征选择的目的是为了去掉不含信息量或是信息量较少的特征，特征选择和方法很多，如果特征上百个可以选择降维、相关系数分析、卡方检验等方法，既然是暴力的撸出一个baseline，就直接肉眼观察剔除不重要的特征。 PassengerId？Name？ 剔除 Ticket 观察一下每张船票都不一样，就跟条形码一样是无用特征 Cabin 缺失值严重，剔除 In [3]: 12345#特征选择data.drop(['Cabin','Name','Ticket','PassengerId'] ,inplace=True ,axis=1 ) 4.2 缺失值处理由于模型本身没有处理缺失值的能力，我们需要人工的处理缺失值。 缺失值处理的方法常见的有均值填充、中位数填充、归为一类新的特征甚至可以用随机森林或者K-means来预测，还是那句话，先撸出一个model来，怎么快怎么来！ Age大部分数据还是完整的（714／891），因此直接上均值填充填充 In [4]: 12#处理缺失值data['Age'] = data['Age'].fillna(data['Age'].mean()) In [5]:1data.info() Out [5]: 123456789101112RangeIndex: 891 entries, 0 to 890Data columns (total 8 columns):Survived 891 non-null int64Pclass 891 non-null int64Sex 891 non-null objectAge 891 non-null float64SibSp 891 non-null int64Parch 891 non-null int64Fare 891 non-null float64Embarked 889 non-null object &lt;--缺失值dtypes: float64(2), int64(4), object(2)memory usage: 55.8+ KB Embarked 的缺失记录只有2条，怎么快怎么来——直接把那两条记录删掉！ In [6]: 12#删除含有空值的记录data = data.dropna(axis=0) In [7]: 1data.info() #再次观察数据 Out [7]: 123456789101112Int64Index: 889 entries, 0 to 890Data columns (total 8 columns):Survived 889 non-null int64Pclass 889 non-null int64Sex 889 non-null objectAge 889 non-null float64SibSp 889 non-null int64Parch 889 non-null int64Fare 889 non-null float64Embarked 889 non-null objectdtypes: float64(2), int64(4), object(2)memory usage: 62.5+ KB 非常干净了，缺失值的处理到此为止！ 4.3 数据转换数据转换的目的就是把人看的数据转换成计算机看得懂的数据。 sklearn的模型无法识别male和female，我们需要用0/1来代替 In [8]: 12#男性为1（True），女性为0(False)data['Sex'] = (data['Sex'] == 'male').astype('int') 再看看Embarked，官方数据集高速我们总共有三个港口分别是C、Q、S 同样的方式处理，映射成0，1，2 In [9]: 1data['Embarked'] = data['Embarked'].map(&#123;'S':0,'C':1,'Q':2&#125;) 再看看现在数据是什么样子 In [10]: 1data.head() Survived Pclass Sex Age SibSp Parch Fare Embarked 0 0 3 1 22.0 1 0 7.2500 0 1 1 1 0 38.0 1 0 71.2833 1 2 1 3 0 26.0 0 0 7.9250 0 3 1 1 0 35.0 1 0 53.1000 0 4 0 3 1 35.0 0 0 8.0500 0 到这里一个简单的数据预处理就结束了，没有过多的数据分析，仅仅是把数据处理成模型能够处理的格式。 5. 决策树分类到了这里就是真正的运用机器学习算法了。 第一步，把数据调整成sklearn能够传入的格式： sklearn的模型都是把特征和标签分别传入训练，否则一整个数据集模型也无法得知哪个才是特征哪个是标签 In [11]: 12X = data.iloc[:,data.columns != "Survived"]y = data.iloc[:,data.columns == "Survived"] 第二步，划分训练集和测试集： 我们把训练集和测试集按7:3 进行划分 In [12]: 12345from sklearn.tree import DecisionTreeClassifierfrom sklearn.model_selection import train_test_splitfrom sklearn.model_selection import cross_val_scoreXtrain,Xtest,Ytrain,Ytest = train_test_split(X,y,test_size=0.3)#test_size是测试集占总数据集的比例 第三步，导入模型，粗略跑一下查看结果： sklearn的模型运用基本上分为三步：调用模型、训练模型、评价模型。三行代码如下。 In [13]: 123456789#1.声明分类树模型clf = DecisionTreeClassifier()#2.传入训练集训练模型clf = clf.fit(Xtrain, Ytrain)#3.传入测试集评价模型score_ = clf.score(Xtest, Ytest)score = cross_val_score(clf,X,y,cv=10).mean() #交叉验证集准确度print('测试集准确度:&#123;&#125;\n交叉验证集准确度:&#123;&#125;'.format(score_,score)) Out [13]: 12测试集准确度:0.7715355805243446交叉验证集准确度:0.7717058222676201 以上，一个非常粗略的baseline就撸出来了。 6. 模型参数调整上面那个粗略的分类树模型都是用默认参数，简单方便但是效果确不是很好，至少调整一个合适的参数还是能够继续提高准确度。 6.1 DecisionTreeClassifier参数介绍调参，我们首先要知道有哪些参数以及参数的含义。这里就先列出分类树常用的参数 参数=默认 介绍 criterion=’gini’ gini／entropy 划分节点的指标 splitter=’best’ 节点分支策略 max_depth=’None’ 树最大深度 min_samples_split=2 一个中间节点分支需要的最少样本（&lt;min_samples_split就不分枝） min_samples_leaf=1 分支后叶节点至少需要的最少样本 random_state 随机数种子 可以通过试不同的变量来确定一部分参数 6.2 初步尝试In [14]: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#分别记录不同参数在测试集和训练集下准确度import matplotlib.pyplot as plttr_entropy = []te_entropy = []tr_gini = []te_gini = []#尝试深度从1～10for i in range(10): clf = DecisionTreeClassifier(random_state=25 ,max_depth=i+1 ,criterion='entropy' #尝试信息增益 ) clf.fit(Xtrain,Ytrain) score_tr = clf.score(Xtrain,Ytrain) score_te = cross_val_score(clf,X,y,cv=10).mean() tr_entropy.append(score_tr) te_entropy.append(score_te) clf = DecisionTreeClassifier(random_state=25 ,max_depth=i+1 ,criterion='gini' #尝试基尼系数 ) clf.fit(Xtrain,Ytrain) score_tr = clf.score(Xtrain,Ytrain) score_te = cross_val_score(clf,X,y,cv=10).mean() tr_gini.append(score_tr) te_gini.append(score_te) fig, (ax0, ax1) = plt.subplots(1,2, figsize=(18, 6)) ax0.plot(range(1,11),tr_entropy,color='r',label='train')ax0.plot(range(1,11),te_entropy,color='blue',label='test')ax0.set_xticks(range(1,11))ax0.set_title('entropy')ax0.legend()ax1.plot(range(1,11),tr_gini,color='r',label='train')ax1.plot(range(1,11),te_gini,color='blue',label='test')ax1.set_xticks(range(1,11))ax0.set_title('gini')ax1.legend()print('entropy上的最好准确度为&#123;&#125;\njini上的最好准确度为&#123;&#125;'.format(max(te_entropy),max(te_gini))) Out [14]: 12entropy上的最好准确度为0.8177860061287026jini上的最好准确度为0.8177987742594486 比起默认参数，经过参数的粗略调整后，模型在测试集上的准确度得到了明显提升 可以观察出当最大深度为3时，拟合效果较好，且两种划分情况准确度都十分相近 6.3 网格搜索调整参数如果参数的取值范围很大，参数个数也很多，这么一个个参数人为的去慢慢尝试是非常消耗时间的，因此我们可以调用sklearn的GridSearchCV来帮助我们寻找合适的参数。 网格参数搜索的本质其实就是把每个参数的取值排列组合一个个帮我们尝试，并且返回交叉验证准确度最好的一组参数。 在调用网格参数搜索前最好先确定参数的大致范围，否则相当消耗时间 In [15]: 123456789101112131415161718192021#网格搜索：能够帮助我们调整多个参数的技术---枚举#网格搜索：能够帮助我们调整多个参数的技术---枚举import numpy as npfrom sklearn.model_selection import GridSearchCVgini_threholds = np.linspace(0,0.5,20)parameters = &#123;'criterion':('gini','entropy') ,'splitter':('best','random') ,'max_depth':[*range(2,5)] ,'min_samples_leaf':[*range(1,10,2)] # ,'min_impurity_decrease':np.linspace(0,0.5,20) &#125;clf = DecisionTreeClassifier(random_state=25)gs = GridSearchCV(clf,parameters,cv=10)gs.fit(Xtrain,Ytrain) In [16]: 1gs.best_params_ #我们输入参数和参数取值中，最佳组合 Out [16]: 1234&#123;&apos;criterion&apos;: &apos;gini&apos;, &apos;max_depth&apos;: 4, &apos;min_samples_leaf&apos;: 1, &apos;splitter&apos;: &apos;random&apos;&#125; 用训练的参数导入模型 In [17]: 12345678clf = DecisionTreeClassifier(random_state=20 ,criterion='gini' ,max_depth=4 ,min_samples_leaf=1 ,splitter='random' )clf = clf.fit(Xtrain, Ytrain)cross_val_score(clf,X,y,cv=10).mean() Out [17]: 10.806511746680286 比之前稍差了点，这其实是因为GridSearchCV在评判的参数好坏的标准是把传入的训练集又分为了训练集和测试集，并通过交叉验证求平均找出准确率最好的参数组合；而之前的算法的准确率是直接用训练集训练并用全部数据集交叉验证的结果，因此两者在评判对象上有所不同，如果两者准确率相差不大，那就任选即可。 如果上面的解释没看懂，那就记住如果自己调试的参数和网格搜索结果相差不大，那说明你已经逼近了调参结果的上限，任选一个就好了。 7. 上传到kaggle查看得分把官方的测试数据集进行预测并上传到官网 刚刚的模型是训练集经过处理才能使用的，因此测试集也要做同样处理。 In [18]: 12test = pd.read_csv('Taitanic data/test.csv')test.info() Out [18]: 123456789101112131415RangeIndex: 418 entries, 0 to 417Data columns (total 11 columns):PassengerId 418 non-null int64Pclass 418 non-null int64Name 418 non-null objectSex 418 non-null objectAge 332 non-null float64 &lt;----缺失SibSp 418 non-null int64Parch 418 non-null int64Ticket 418 non-null objectFare 417 non-null float64 &lt;----缺失Cabin 91 non-null objectEmbarked 418 non-null objectdtypes: float64(2), int64(4), object(5)memory usage: 36.0+ KB 发现和训练集不同的是‘Fare’特征有一个缺失值，这需要小心不能忘了处理 In [19]: 1234567891011#把测试集预处理操作封装def clean_data(data): data = data.drop(['Cabin','Name','Ticket','PassengerId'] ,axis=1 ) data['Age'] = data['Age'].fillna(data['Age'].mean()) data['Fare'] = data['Fare'].fillna(data['Fare'].mean()) # data = data.dropna(axis=0) data['Sex'] = (data['Sex'] == 'male').astype('int') data['Embarked'] = data['Embarked'].map(&#123;'S':0,'C':1,'Q':2&#125;) return data In [20]: 1234test_data = clean_data(test)res = pd.concat([test['PassengerId'],pd.DataFrame(clf.predict(test_data))],axis=1)res.columns = ['PassengerId','Survived']res.to_csv("result.csv",sep=',',index=False) 提交结果查看得分，top20%的baseline，还行 8 总结完成了一次完整的kaggle还是很有成就感的，不过依然有很多瑕疵。 kaggle最重要的特征工程几乎被我一笔带过，数据没有经过严密的统计分析，有句话叫“特征工程决定了最后结果的上限，而机器学习算法只是在逼近这个上限”。特征上还有很多事情可以做，例如： Age可以尝试Random forest、SVM等算法预测填充 Cabin可以保留，把缺失值当作一类，非缺失值当作一类 sibsp，parch也可以推测出一个人的年龄区间 sibsp，parch两个特征可以用一个新的特征“家庭成员数量”代替试试 … … 甚至尝试不同的模型，对于不同的模型又会有不同的数据处理方式，例如降为、归一化、One-hot编码等，如果把泰坦尼克号数据集的内容吃透对于其他数据集也能得心应手了。]]></content>
      <tags>
        <tag>kaggle</tag>
        <tag>机器学习，决策树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python向量化思维编程的总结]]></title>
    <url>%2F2019%2F01%2F25%2FVectorization%2F</url>
    <content type="text"><![CDATA[我的python为什么比较慢刚刚开始接触python时，以为学习了基本的语法和数据结构以及常用包的API就算是掌握了这门语言，但是写起算法时，除了感受到语法上的精简外也看不到这门语言的高效之处。 有一个很大的原因就是沿用了以前java／c的编程思维，但是python处理数据所面对的问题常常需要大量的迭代、累加和样本的重复计算，用c语言的编程习惯很容易上来就是for循环，例如写矩阵的乘法用暴力三次方的复杂度来解决. 123456789101112131415//矩阵乘法，3个for循环搞定 void MulMatrix(int** matrixA, int** matrixB, int** matrixC) &#123; for(int i = 0; i &lt; 2; ++i) &#123; for(int j = 0; j &lt; 2; ++j) &#123; matrixC[i][j] = 0; for(int k = 0; k &lt; 2; ++k) &#123; matrixC[i][j] += matrixA[i][k] * matrixB[k][j]; &#125; &#125; &#125; &#125; 如果在python上还沿用这种思维，那仅仅是换个语法重新实现这个算法罢了，完全没有发挥出python的优势，甚至用C写的效率还会更高。 python之所以在数据分析上有它的一席之地，是因为他快。如何发挥出它的效率，那就需要向量化的编程思维。 利用python专门处理向量／矩阵运算的包——numpy，支持大量的维度数组与矩阵运算，此外也针对数组运算提供大量的数学函数库。 向量化编程的例子例1: h_\theta(x) = \theta_0+\theta_1x_1+\theta_2x_2+\theta_3x_3令：$\theta=[\theta_0,\theta_1,\theta_2,\theta_3]^T$；$x=[1,x_1,x_2,x_3.x_4]^T$ 可以写成： h_\theta(x)=\theta^Tx1234#pythont = np.array([t1,t2,t3,t4])x = np.array([1,x1,x2,x3,x4])h = t.T @ x # '@'相当于向量相乘 例2: J(\theta) = \frac{1}{m}\sum_{i=1}^{m}[-y^{(i)}log(h_{\theta}(x^{(i)}))-(1-y^{(i)})log(1-h_{\theta}(x^{(i)}))]1234567891011#各变量维度#x:(m,n)#y:(m,1)#theta:(n,1)def computer_cost(theta,x,y): m = len(x) h = sigmoid(x@theta) # (m,n)*(n,1)=(m,1) first = np.log(h).T @ y #(m,1).T*(m,1) = (1,m)*(m,1)=(1,1) second = np.log(1-h).T @ (1-y) cost = -(first+second)/m return cost]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
</search>
